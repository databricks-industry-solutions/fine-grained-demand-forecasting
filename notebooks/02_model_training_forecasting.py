# Databricks notebook source
# MAGIC %md 
# MAGIC # AI-Powered Demand Forecasting - Intelligent Prediction Engine
# MAGIC 
# MAGIC ## ü§ñ Advanced Analytics Solution
# MAGIC 
# MAGIC Transform your retail operations with **intelligent demand prediction** that learns from historical patterns:
# MAGIC - **AI-driven forecasting** across thousands of store-product combinations
# MAGIC - **Seasonal intelligence** that adapts to holiday patterns and market trends
# MAGIC - **Risk-aware predictions** with confidence intervals for safety stock planning
# MAGIC - **Automated model training** that scales to enterprise product catalogs
# MAGIC 
# MAGIC ## üìà Industry Innovation
# MAGIC 
# MAGIC ```
# MAGIC ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
# MAGIC ‚îÇ  Sales History  ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  AI Forecasting ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ Business Actions‚îÇ
# MAGIC ‚îÇ  ‚úÖ Foundation  ‚îÇ    ‚îÇ üîÑ Current step ‚îÇ    ‚îÇ  ‚è≥ Coming next ‚îÇ
# MAGIC ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
# MAGIC ```
# MAGIC 
# MAGIC ## üéØ Smart Forecasting Capabilities
# MAGIC 
# MAGIC - **Pattern Recognition**: Automatically detects seasonal, weekly, and daily trends
# MAGIC - **Demand Volatility Management**: Handles irregular patterns and market disruptions
# MAGIC - **Uncertainty Quantification**: Provides confidence ranges for inventory planning
# MAGIC - **Scale & Performance**: Processes thousands of products simultaneously
# MAGIC - **Business Intelligence**: Generates actionable insights for supply chain teams

# COMMAND ----------

# MAGIC %md ## üì¶ AI/ML Environment Setup

# COMMAND ----------

# DBTITLE 1,Install Required Libraries
# Install libraries for serverless compute
# MAGIC %pip install prophet>=1.1.5 plotly>=5.17.0 scikit-learn>=1.3.0

# COMMAND ----------

# Restart Python to use newly installed libraries
dbutils.library.restartPython()

# COMMAND ----------

# MAGIC %md ## ‚öôÔ∏è Forecasting Engine Configuration

# COMMAND ----------

# DBTITLE 1,Import Required Libraries
import pandas as pd
import numpy as np
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, count, max as spark_max, min as spark_min, current_timestamp
from pyspark.sql.functions import pandas_udf, PandasUDFType
from pyspark.sql.types import StructType, StructField, DateType, DoubleType, IntegerType, StringType
from prophet import Prophet
from datetime import datetime, timedelta
import warnings
warnings.filterwarnings('ignore')

print("üìö Libraries imported successfully")

# COMMAND ----------

# DBTITLE 1,Configure Forecasting Parameters
# Get parameters from job or use defaults
catalog_name = dbutils.widgets.get("catalog_name") if dbutils.widgets.get("catalog_name") else "dev_demand_forecasting"
schema_name = dbutils.widgets.get("schema_name") if dbutils.widgets.get("schema_name") else "forecasting"

# Forecasting parameters
FORECAST_HORIZON_DAYS = 30
MIN_HISTORY_DAYS = 90
CONFIDENCE_INTERVAL = 0.95
MODEL_VERSION = "prophet_v1.1.5_serverless"

# Process all generated data for comprehensive forecasting
MAX_STORES = 10   # Match data generation: stores 1-10
MAX_ITEMS = 50    # Match data generation: items 1-50

print("üîß AI Forecasting Engine Settings:")
print(f"   üîÆ Forecast horizon: {FORECAST_HORIZON_DAYS} days ahead")
print(f"   üìä Minimum sales history: {MIN_HISTORY_DAYS} days")
print(f"   üéØ Prediction confidence: {CONFIDENCE_INTERVAL * 100}%")
print(f"   üõí Retail scope: {MAX_STORES} stores √ó {MAX_ITEMS} products")
print(f"   ü§ñ Processing: Intelligent auto-scaling")

# Set up Spark session
spark = SparkSession.builder.getOrCreate()

# COMMAND ----------

# MAGIC %md ## üìä Retail Sales Data Analysis
# MAGIC 
# MAGIC ### Historical Sales Pattern Analysis
# MAGIC Loading and analyzing your retail sales history to identify forecasting opportunities and data quality.

# COMMAND ----------

# DBTITLE 1,Load Sales Data from Unity Catalog
print("üì• Loading retail sales history for AI analysis...")

# Load historical sales data
raw_table = f"{catalog_name}.{schema_name}.raw_sales_data"
df = spark.table(raw_table)

print(f"‚úÖ Sales data ready for analysis")
print(f"üõí Total sales transactions: {df.count():,}")

# Data quality summary
date_range = df.select(spark_min('date'), spark_max('date')).collect()[0]
print(f"üìÖ Date range: {date_range[0]} to {date_range[1]}")
print(f"üè™ Stores: {df.select('store').distinct().count()}")
print(f"üì¶ Items: {df.select('item').distinct().count()}")

# COMMAND ----------

# DBTITLE 1,Validate Data for Forecasting
print("üîç Analyzing sales patterns for AI model training...")

# Check data completeness for selected store-item combinations
validation_df = (
    df.filter(col("store") <= MAX_STORES)
    .filter(col("item") <= MAX_ITEMS)
    .groupBy("store", "item")
    .agg(
        count("*").alias("record_count"),
        spark_min("date").alias("start_date"),
        spark_max("date").alias("end_date")
    )
)

validation_results = validation_df.collect()

print(f"üìà Analyzing {len(validation_results)} store-item combinations:")

sufficient_data_count = 0
for row in validation_results:
    days_of_data = (row['end_date'] - row['start_date']).days + 1
    sufficient = days_of_data >= MIN_HISTORY_DAYS
    if sufficient:
        sufficient_data_count += 1
    
    status = "‚úÖ" if sufficient else "‚ùå"
    print(f"   {status} Store {row['store']}, Item {row['item']}: {row['record_count']} records, {days_of_data} days")

print(f"\nüéØ {sufficient_data_count}/{len(validation_results)} product-store combinations ready for AI forecasting")

# COMMAND ----------

# MAGIC %md ## üß† AI-Powered Demand Prediction Engine
# MAGIC 
# MAGIC ### Intelligent Forecasting at Scale
# MAGIC 
# MAGIC Deploy advanced machine learning to predict demand across your entire product portfolio.
# MAGIC Our AI engine will:
# MAGIC 1. **Learn from sales patterns** (seasonality, trends, promotions)
# MAGIC 2. **Train product-specific models** optimized for each store-item combination
# MAGIC 3. **Generate intelligent forecasts** with uncertainty quantification
# MAGIC 4. **Scale automatically** to handle enterprise product catalogs

# COMMAND ----------

# DBTITLE 1,Define Prophet Forecasting Function
@pandas_udf(returnType=StructType([
    StructField("store", IntegerType()),
    StructField("item", IntegerType()),
    StructField("forecast_date", DateType()),
    StructField("yhat", DoubleType()),
    StructField("yhat_lower", DoubleType()),
    StructField("yhat_upper", DoubleType()),
    StructField("model_version", StringType())
]), functionType=PandasUDFType.GROUPED_MAP)
def forecast_store_item(pdf):
    """
    Train Prophet model and generate forecasts for a single store-item combination
    
    Args:
        pdf (pandas.DataFrame): Historical sales data for one store-item combination
        
    Returns:
        pandas.DataFrame: Forecast results with confidence intervals
    """
    try:
        # Validate input data
        if len(pdf) < MIN_HISTORY_DAYS:
            print(f"Insufficient data: Store {pdf['store'].iloc[0]}, Item {pdf['item'].iloc[0]} has only {len(pdf)} days")
            return pd.DataFrame()
        
        # Prepare data for Prophet (requires 'ds' and 'y' columns)
        prophet_df = pdf[['date', 'sales']].rename(columns={'date': 'ds', 'sales': 'y'})
        prophet_df['ds'] = pd.to_datetime(prophet_df['ds'])
        prophet_df = prophet_df.sort_values('ds')
        
        # Remove any duplicate dates (if they exist)
        prophet_df = prophet_df.drop_duplicates(subset=['ds'])
        
        # Create and configure Prophet model
        model = Prophet(
            daily_seasonality=True,      # Capture day-of-week patterns
            weekly_seasonality=True,     # Capture weekly patterns  
            yearly_seasonality=True,     # Capture seasonal patterns
            interval_width=CONFIDENCE_INTERVAL,  # Set confidence interval width
            changepoint_prior_scale=0.05,       # Controls trend flexibility
            seasonality_prior_scale=10.0        # Controls seasonality strength
        )
        
        # Fit the model
        model.fit(prophet_df)
        
        # Create future dataframe for forecasting
        future = model.make_future_dataframe(periods=FORECAST_HORIZON_DAYS)
        
        # Generate forecast
        forecast = model.predict(future)
        
        # Filter to only future dates (exclude historical fitted values)
        last_historical_date = prophet_df['ds'].max()
        future_forecast = forecast[forecast['ds'] > last_historical_date].copy()
        
        # Prepare output with store/item identifiers
        result = pd.DataFrame({
            'store': pdf['store'].iloc[0],
            'item': pdf['item'].iloc[0],
            'forecast_date': future_forecast['ds'].dt.date,
            'yhat': future_forecast['yhat'],
            'yhat_lower': future_forecast['yhat_lower'],
            'yhat_upper': future_forecast['yhat_upper'],
            'model_version': MODEL_VERSION
        })
        
        # Ensure non-negative forecasts (sales can't be negative)
        result['yhat'] = result['yhat'].clip(lower=0)
        result['yhat_lower'] = result['yhat_lower'].clip(lower=0)
        result['yhat_upper'] = result['yhat_upper'].clip(lower=0)
        
        return result
        
    except Exception as e:
        print(f"‚ùå Error forecasting Store {pdf['store'].iloc[0]}, Item {pdf['item'].iloc[0]}: {str(e)}")
        return pd.DataFrame()

print("‚úÖ AI demand prediction engine ready")

# COMMAND ----------

# MAGIC %md ## üöÄ Deploy AI Forecasting at Enterprise Scale
# MAGIC 
# MAGIC ### Intelligent Demand Prediction Across Your Retail Portfolio
# MAGIC 
# MAGIC Launch AI-powered forecasting across your stores and products. The system automatically trains specialized models for each product-location combination.

# COMMAND ----------

# DBTITLE 1,Run Distributed Prophet Forecasting
print("üöÄ Launching AI-powered demand forecasting...")

# Get all available store-item combinations dynamically from the data
available_combinations = (
    df.select("store", "item")
    .distinct()
    .collect()
)

print(f"üéØ Discovered {len(available_combinations)} store-item combinations in data")

# Create forecast results storage
all_forecasts = []

# Process each combination individually for better error handling
for i, row in enumerate(available_combinations):
    store_id = row['store']
    item_id = row['item']
    
    try:
        # Filter data for this specific store-item combination
        store_item_data = (
            df.filter((col("store") == store_id) & (col("item") == item_id))
            .select("date", "sales")
            .orderBy("date")
            .toPandas()
        )
        
        # Check if we have enough data
        if len(store_item_data) < MIN_HISTORY_DAYS:
            print(f"‚ö†Ô∏è  Store {store_id}, Item {item_id}: Only {len(store_item_data)} days (need {MIN_HISTORY_DAYS})")
            continue
            
        # Prepare data for Prophet
        prophet_df = store_item_data.rename(columns={'date': 'ds', 'sales': 'y'})
        prophet_df['ds'] = pd.to_datetime(prophet_df['ds'])
        prophet_df = prophet_df.sort_values('ds').drop_duplicates(subset=['ds'])
        
        # Train Prophet model
        model = Prophet(
            daily_seasonality=True,
            weekly_seasonality=True,
            yearly_seasonality=True,
            interval_width=CONFIDENCE_INTERVAL,
            changepoint_prior_scale=0.05,
            seasonality_prior_scale=10.0
        )
        
        # Suppress Prophet logging
        import logging
        logging.getLogger('prophet').setLevel(logging.WARNING)
        
        model.fit(prophet_df)
        
        # Generate forecasts
        future = model.make_future_dataframe(periods=FORECAST_HORIZON_DAYS)
        forecast = model.predict(future)
        
        # Get only future predictions
        last_date = prophet_df['ds'].max()
        future_forecast = forecast[forecast['ds'] > last_date].copy()
        
        # Prepare results
        for _, forecast_row in future_forecast.iterrows():
            all_forecasts.append({
                'store': int(store_id),
                'item': int(item_id),
                'forecast_date': forecast_row['ds'].date(),
                'yhat': max(0, float(forecast_row['yhat'])),
                'yhat_lower': max(0, float(forecast_row['yhat_lower'])),
                'yhat_upper': max(0, float(forecast_row['yhat_upper'])),
                'model_version': MODEL_VERSION
            })
        
        if (i + 1) % 50 == 0:  # Progress update every 50 combinations
            print(f"üìà Processed {i + 1}/{len(available_combinations)} combinations...")
            
    except Exception as e:
        print(f"‚ùå Error with Store {store_id}, Item {item_id}: {str(e)}")
        continue

print(f"‚úÖ Forecasting complete! Generated predictions for {len(set([(f['store'], f['item']) for f in all_forecasts]))} combinations")

# Convert to Spark DataFrame
if all_forecasts:
    forecast_df = spark.createDataFrame(all_forecasts)
    forecast_count = forecast_df.count()
    print(f"üîÆ Generated {forecast_count:,} individual demand predictions")
else:
    forecast_df = None
    forecast_count = 0
    print("‚ùå No forecasts generated")

# COMMAND ----------

# DBTITLE 1,Validate Forecast Results
if forecast_count > 0:
    print("‚úÖ AI-powered demand forecasting completed successfully!")
    
    # Business impact summary
    unique_combinations = forecasts.select("store", "item").distinct().count()
    date_range = forecasts.select(spark_min("forecast_date"), spark_max("forecast_date")).collect()[0]
    
    print(f"üõí Products with AI forecasts: {unique_combinations} store-product combinations")
    print(f"üìÖ Demand predictions through: {date_range[1]}")
    print(f"üìä Daily predictions per product: {forecast_count // unique_combinations}")
    
    # Sample business forecasts
    sample_forecasts = forecasts.select("store", "item", "forecast_date", "yhat", "yhat_lower", "yhat_upper").limit(5).collect()
    print("\nüìã Sample demand predictions:")
    for row in sample_forecasts:
        print(f"   Store {row['store']}, Product {row['item']}, {row['forecast_date']}: {row['yhat']:.0f} units (range: {row['yhat_lower']:.0f}-{row['yhat_upper']:.0f})")
        
else:
    print("‚ùå No demand forecasts generated - review data requirements")

# COMMAND ----------

# DBTITLE 1,Save Forecasts to Unity Catalog
if forecast_count > 0:
    print("üíæ Saving demand forecasts to Unity Catalog...")
    
    # Add business metadata
    forecasts_with_timestamp = forecast_df.withColumn("created_timestamp", current_timestamp())
    
    # Save to table
    forecast_table = f"{catalog_name}.{schema_name}.forecast_results"
    forecasts_with_timestamp.write.mode("overwrite").saveAsTable(forecast_table)
    
    print(f"‚úÖ Saved {forecast_count:,} demand predictions to {forecast_table}")
    
    # Verify save
    saved_df = spark.table(forecast_table)
    saved_count = saved_df.count()
    unique_combinations = saved_df.select("store", "item").distinct().count()
    date_range = saved_df.select(spark_min("forecast_date"), spark_max("forecast_date")).collect()[0]
    
    print(f"üìä Verified: {saved_count:,} forecasts for {unique_combinations} store-item combinations")
    print(f"üìÖ Forecast period: {date_range[0]} to {date_range[1]}")
    
else:
    print("‚ö†Ô∏è  No forecasts to save")

print(f"üéØ Forecast generation complete: {forecast_count:,} predictions ready for business use")

# COMMAND ----------

# MAGIC %md ## üìä AI Forecasting Performance Summary
# MAGIC 
# MAGIC ### Business Impact Assessment

# COMMAND ----------

# DBTITLE 1,Generate Performance Report
print("üìä AI Forecasting Business Impact Report")
print("=" * 50)

if forecast_count > 0:
    # Load saved forecasts for analysis
    forecast_table = f"{catalog_name}.{schema_name}.forecast_results"
    results_df = spark.table(forecast_table)
    
    # Performance metrics
    total_models = results_df.select("store", "item").distinct().count()
    total_forecasts = results_df.count()
    avg_forecast_value = results_df.agg({"yhat": "avg"}).collect()[0][0]
    
    print(f"üõí Products with AI forecasts: {total_models}")
    print(f"üîÆ Total demand predictions: {total_forecasts:,}")
    print(f"üìä Average daily demand forecast: {avg_forecast_value:.0f} units")
    print(f"ü§ñ AI Engine: Advanced time series modeling")
    print(f"‚ö° Processing: Enterprise auto-scaling platform")
    print(f"üìÖ Planning horizon: {FORECAST_HORIZON_DAYS} days ahead")
    
    # Forecast range analysis
    forecast_stats = results_df.select("yhat", "yhat_lower", "yhat_upper").describe().collect()
    print(f"\nüìà Forecast Value Distribution:")
    for stat in forecast_stats:
        if stat['summary'] in ['mean', 'min', 'max', 'stddev']:
            print(f"   {stat['summary'].capitalize()}: {float(stat['yhat']):.2f}")
    
    print(f"\n‚úÖ AI demand forecasting engine operational!")
    
else:
    print("‚ùå No AI models were successfully trained")
    print("   Review sales data requirements and training logs")

# COMMAND ----------

# MAGIC %md ## üìã Summary & Next Steps
# MAGIC 
# MAGIC ### ‚úÖ AI Forecasting Engine Deployed:
# MAGIC 
# MAGIC 1. **üß† Advanced AI Models**: Specialized demand prediction for each product-store
# MAGIC 2. **üîÆ Intelligent Forecasts**: 30-day demand predictions with confidence ranges  
# MAGIC 3. **‚ö° Enterprise Scale**: Automatic scaling across thousands of products
# MAGIC 4. **üíæ Business Integration**: Forecasts available to all planning teams
# MAGIC 5. **üìä Quality Assurance**: Validated predictions ready for inventory planning
# MAGIC 
# MAGIC ### üîÑ Business Intelligence Next:
# MAGIC 
# MAGIC **Final Step**: Executive Insights & Action Planning
# MAGIC - Interactive demand visualization dashboards
# MAGIC - Inventory optimization recommendations
# MAGIC - Supply chain planning insights
# MAGIC - Executive KPI reporting and ROI analysis
# MAGIC 
# MAGIC ### üìç Output Location:
# MAGIC ```
# MAGIC Catalog: {catalog_name}
# MAGIC Schema: {schema_name}  
# MAGIC Table: forecast_results
# MAGIC Records: {forecast_count:,} forecasts
# MAGIC ```

# COMMAND ----------

# MAGIC %md ## üéâ AI Forecasting Engine Live!
# MAGIC 
# MAGIC **Your intelligent demand prediction system is operational!**
# MAGIC 
# MAGIC Ready to transform forecasts into actionable business insights and inventory optimization strategies.

# COMMAND ----------

# Return completion status for workflow orchestration
completion_message = f"SUCCESS: {forecast_count} demand predictions generated for {unique_combinations if forecast_count > 0 else 0} product-store combinations"
dbutils.notebook.exit(completion_message) 